{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7946356,"sourceType":"datasetVersion","datasetId":4672524},{"sourceId":7968117,"sourceType":"datasetVersion","datasetId":4688293}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet50\nfrom torch.optim import Adam\nimport h5py\nimport imageio as img\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import Dataset\nclass MyDataset(Dataset):\n    def __init__(self, h5_file, transform=None):\n        self.file = h5py.File(h5_file, 'r')\n        self.transform = transform\n        self.X = self.file['jet']\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        image = self.X[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image\ndef augment(x):\n    x = x.numpy()\n    # Define the augmentation\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.GaussianBlur(kernel_size=3),\n        transforms.RandomRotation(degrees=10),\n        transforms.ToTensor()\n    ])\n    y_a = torch.stack([transform(transforms.ToPILImage()(channel)) for img in x for channel in img])\n    y_b = torch.stack([transform(transforms.ToPILImage()(channel)) for img in x for channel in img])\n\n    # Reshape the output to match the original shape\n    y_a = y_a.view(*x.shape)\n    y_b = y_b.view(*x.shape)\n    return y_a, y_b\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResBlock, self).__init__()\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        identity = self.shortcut(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x += identity\n        x = self.relu(x)\n        return x\n\nclass ResNet15(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet15, self).__init__()\n        self.conv0 = nn.Conv2d(8, 120, kernel_size=125, stride=2, padding=3, bias=False)\n        self.bn0 = nn.BatchNorm2d(120)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.res_blocks = nn.Sequential(\n            ResBlock(120, 120),\n            ResBlock(120, 120),\n            ResBlock(120, 90, stride=2),\n            ResBlock(90, 60),\n            ResBlock(60, 30, stride=2),\n            ResBlock(30, 30)\n        )\n\n        self.conv_final = nn.Conv2d(30, 16, kernel_size=3, stride=2, padding=1)\n        self.bn_final = nn.BatchNorm2d(16)\n        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(16, num_classes)\n\n    def forward(self, x):\n        x = self.conv0(x)\n        x = self.bn0(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.res_blocks(x)\n        x = self.conv_final(x)\n        x = self.bn_final(x)\n        x = self.relu(x)\n        x = self.global_avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# Create the model\nmodel = ResNet15(num_classes=1)\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:28:49.562259Z","iopub.execute_input":"2024-04-02T17:28:49.562863Z","iopub.status.idle":"2024-04-02T17:28:49.842351Z","shell.execute_reply.started":"2024-04-02T17:28:49.562822Z","shell.execute_reply":"2024-04-02T17:28:49.840817Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"ResNet15(\n  (conv0): Conv2d(8, 120, kernel_size=(125, 125), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (res_blocks): Sequential(\n    (0): ResBlock(\n      (shortcut): Sequential()\n      (conv1): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): ResBlock(\n      (shortcut): Sequential()\n      (conv1): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): ResBlock(\n      (shortcut): Conv2d(120, 90, kernel_size=(1, 1), stride=(2, 2))\n      (conv1): Conv2d(120, 90, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): ResBlock(\n      (shortcut): Conv2d(90, 60, kernel_size=(1, 1), stride=(1, 1))\n      (conv1): Conv2d(90, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): ResBlock(\n      (shortcut): Conv2d(60, 30, kernel_size=(1, 1), stride=(2, 2))\n      (conv1): Conv2d(60, 30, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): ResBlock(\n      (shortcut): Sequential()\n      (conv1): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (conv_final): Conv2d(30, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (bn_final): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n  (fc): Linear(in_features=16, out_features=1, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Lars Optimizer","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.optim import Optimizer\n\nclass LARS(Optimizer):\n    def __init__(self, params, lr=0.1, trust_coef=0.001, eps=1e-8):\n        defaults = dict(lr=lr, trust_coef=trust_coef, eps=eps)\n        super(LARS, self).__init__(params, defaults)\n\n    def step(self):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                global_lr = group['lr']\n                velocity = global_lr * p.grad.data\n\n                trust_coef = group['trust_coef']\n                eps = group['eps']\n\n                param_norm = torch.norm(p.data)\n                grad_norm = torch.norm(p.grad.data)\n\n                local_lr = trust_coef * param_norm / (grad_norm + param_norm * eps)\n                adjusted_lr = min(local_lr, global_lr)\n\n                velocity.add_(adjusted_lr, p.data)\n                p.data.add_(-velocity)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:11:56.753145Z","iopub.execute_input":"2024-04-02T12:11:56.753723Z","iopub.status.idle":"2024-04-02T12:11:56.765333Z","shell.execute_reply.started":"2024-04-02T12:11:56.753690Z","shell.execute_reply":"2024-04-02T12:11:56.764270Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Barlow Twins Custom Loss Function","metadata":{}},{"cell_type":"code","source":"\ntransform = transforms.Compose([\n\n    transforms.ToTensor(),\n])\n\n# Wrap the SGD optimizer with LARS\noptimizer = LARS(model.parameters(), lr=0.0001, trust_coef=0.001)\ntrainset = MyDataset('/kaggle/input/datqwe/Dataset_Specific_Unlabelled.h5',transform=transform)\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n\noutput_dim = 512\nlambda_=0.01\nepoch=0\n# f: encoder network # lambda: weight on the off-diagonal terms # N: batch size # D: dimensionality of the embeddings # # mm: matrix-matrix multiplication # off_diagonal: off-diagonal elements of a matrix # eye: identity matrix \nfor i in range(2):\n    epoch+=1\n    epoch_loss = 0.0\n\n    for x in trainloader: # load a batch with N samples # two randomly augmented versions of x \n        y_a, y_b = augment(x) # compute embeddings \n        z_a = model(y_a)  # NxD\n        z_b = model(y_b)  # NxD\n \n    # Normalize repr. along the batch dimension\n        z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0)  # NxD\n        z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0)  # NxD\n\n    # Cross-correlation matrix\n        c = torch.mm(z_a_norm.T, z_b_norm) / x.size(0)  # DxD\n\n    # Loss\n        c_diff = (c - torch.eye(output_dim)).pow(2)  # DxD\n\n    # Multiply off-diagonal elems of c_diff by lambda\n        c_diff_off_diag = c_diff - torch.diag(torch.diag(c_diff))\n        c_diff_off_diag.mul_(lambda_)\n        c_diff = torch.diag(torch.diag(c_diff)) + c_diff_off_diag\n\n        loss = c_diff.sum()\n        epoch_loss += loss.item()\n    # Optimization step\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    print(f'Epoch {epoch}, Loss: {epoch_loss / len(trainloader)}')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T12:11:56.767186Z","iopub.execute_input":"2024-04-02T12:11:56.767853Z","iopub.status.idle":"2024-04-02T14:51:47.769907Z","shell.execute_reply.started":"2024-04-02T12:11:56.767820Z","shell.execute_reply":"2024-04-02T14:51:47.768016Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/1235000129.py:26: UserWarning: This overload of add_ is deprecated:\n\tadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\n\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1519.)\n  velocity.add_(adjusted_lr, p.data)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 493.83897341339826\nEpoch 2, Loss: 486.0946560272022\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'res.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T15:24:25.215788Z","iopub.execute_input":"2024-04-02T15:24:25.216386Z","iopub.status.idle":"2024-04-02T15:24:25.437369Z","shell.execute_reply.started":"2024-04-02T15:24:25.216348Z","shell.execute_reply":"2024-04-02T15:24:25.435888Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport imageio as img\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Open the HDF5 file\nfile = h5py.File('/kaggle/input/qweewq/Dataset_Specific_labelled.h5', 'r')\n# Now you can read datasets from the fil\ndata1 = file['/jet']\ny1 = file['/Y']\nprint(y1[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-02T15:27:26.498280Z","iopub.execute_input":"2024-04-02T15:27:26.500718Z","iopub.status.idle":"2024-04-02T15:27:26.516080Z","shell.execute_reply.started":"2024-04-02T15:27:26.500654Z","shell.execute_reply":"2024-04-02T15:27:26.514493Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[0.]\n","output_type":"stream"}]},{"cell_type":"code","source":"test_x = data1[8000:]\ntrain_x = data1[:8000]\ntest_y = y1[8000:]\ntrain_y = y1[:8000]","metadata":{"execution":{"iopub.status.busy":"2024-04-02T15:43:25.715544Z","iopub.execute_input":"2024-04-02T15:43:25.716136Z","iopub.status.idle":"2024-04-02T15:43:34.071178Z","shell.execute_reply.started":"2024-04-02T15:43:25.716094Z","shell.execute_reply":"2024-04-02T15:43:34.069688Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n\n\ntrain_dataset = CustomDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).long())\ntest_dataset = CustomDataset(torch.from_numpy(test_x).float(), torch.from_numpy(test_y).long())\n\nbatch_size=64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T15:44:08.273928Z","iopub.execute_input":"2024-04-02T15:44:08.274482Z","iopub.status.idle":"2024-04-02T15:44:08.295695Z","shell.execute_reply.started":"2024-04-02T15:44:08.274448Z","shell.execute_reply":"2024-04-02T15:44:08.294433Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Results with Self-training","metadata":{}},{"cell_type":"code","source":"import torch\ncriterion =nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n\nnum_epochs=5\n# Training loop\nfor epoch in range(num_epochs):\n    epoch_loss=0.0\n    for i, (data, labels) in enumerate(train_loader):\n        data=data.permute(0, 3, 2, 1)\n        # Forward pass\n        outputs = model(data)\n\n        loss = criterion(outputs, labels.float())\n        epoch_loss += loss.item()\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for data, labels in test_loader:\n        data=data.permute(0, 3, 2, 1)\n\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\nprint('ROC-AUC score: ', roc_auc_score(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:25:59.062257Z","iopub.execute_input":"2024-04-02T17:25:59.063195Z","iopub.status.idle":"2024-04-02T17:26:21.114156Z","shell.execute_reply.started":"2024-04-02T17:25:59.063154Z","shell.execute_reply":"2024-04-02T17:26:21.112687Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"ROC-AUC score:  0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n\nmae = mean_absolute_error(y_true,y_pred)\nmse = mean_squared_error(y_true,y_pred)\nrmse = np.sqrt(mse)\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:27:37.693915Z","iopub.execute_input":"2024-04-02T17:27:37.695642Z","iopub.status.idle":"2024-04-02T17:27:37.712836Z","shell.execute_reply.started":"2024-04-02T17:27:37.695591Z","shell.execute_reply":"2024-04-02T17:27:37.711418Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Mean Absolute Error (MAE): 0.493\nMean Squared Error (MSE): 0.493\nRoot Mean Squared Error (RMSE): 0.7021395872616784\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Results without Self-Training","metadata":{}},{"cell_type":"code","source":"import torch\ncriterion =nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n\nnum_epochs=5\n# Training loop\nfor epoch in range(num_epochs):\n    epoch_loss=0.0\n    for i, (data, labels) in enumerate(train_loader):\n        data=data.permute(0, 3, 2, 1)\n        # Forward pass\n        outputs = model(data)\n\n        loss = criterion(outputs, labels.float())\n        epoch_loss += loss.item()\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n\n\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for data, labels in test_loader:\n        data=data.permute(0, 3, 2, 1)\n\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\nprint('ROC-AUC score: ', roc_auc_score(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:28:57.793051Z","iopub.execute_input":"2024-04-02T17:28:57.794365Z","iopub.status.idle":"2024-04-02T17:46:08.924055Z","shell.execute_reply.started":"2024-04-02T17:28:57.794318Z","shell.execute_reply":"2024-04-02T17:46:08.922278Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch [1/5], Step [125/125], Loss: 0.43758824467658997\nEpoch [2/5], Step [125/125], Loss: 0.15018409490585327\nEpoch [3/5], Step [125/125], Loss: 0.11488958448171616\nEpoch [4/5], Step [125/125], Loss: 0.18253324925899506\nEpoch [5/5], Step [125/125], Loss: 0.0474543496966362\nROC-AUC score:  0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n\nmae = mean_absolute_error(y_true,y_pred)\nmse = mean_squared_error(y_true,y_pred)\nrmse = np.sqrt(mse)\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-02T17:46:33.333170Z","iopub.execute_input":"2024-04-02T17:46:33.335482Z","iopub.status.idle":"2024-04-02T17:46:33.352977Z","shell.execute_reply.started":"2024-04-02T17:46:33.335422Z","shell.execute_reply":"2024-04-02T17:46:33.351208Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Mean Absolute Error (MAE): 0.493\nMean Squared Error (MSE): 0.493\nRoot Mean Squared Error (RMSE): 0.7021395872616784\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}