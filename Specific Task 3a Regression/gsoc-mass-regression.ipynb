{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7929951,"sourceType":"datasetVersion","datasetId":4660949},{"sourceId":7994458,"sourceType":"datasetVersion","datasetId":4706689}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"markdown","source":"Description: As I didnot have enough RAM to load all the dataset, I was only able to perform computation on the first 10000 rows of the first dataset out of six.\nI will be really pleased if the judge would check my accuracy individually and not compare it with others who have better RAMs installed.","metadata":{}},{"cell_type":"code","source":"import pyarrow.parquet as pq\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pyarrow as pa \nfrom pyarrow.parquet import ParquetFile\n\n# Path to the parquet file\nfile_path = '/kaggle/input/datsat/top_gun_opendata_0.parquet'\n\n# Read the Parquet file\nnum_rows = pq.read_metadata(file_path).num_rows\nprint(num_rows)\n# Read the first half of the Parquet file\npf = ParquetFile('/kaggle/input/datsat/top_gun_opendata_0.parquet') \nfirst_ten_rows = next(pf.iter_batches(batch_size = 10000)) \ndf = pa.Table.from_batches([first_ten_rows]).to_pandas() \n# Print the DataFrame\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:06:33.234304Z","iopub.execute_input":"2024-04-01T09:06:33.234726Z","iopub.status.idle":"2024-04-01T09:07:32.451091Z","shell.execute_reply.started":"2024-04-01T09:06:33.234693Z","shell.execute_reply":"2024-04-01T09:07:32.449945Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"150327\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                                  X_jet           m  iphi  \\\n0     [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  291.988312  33.0   \n1     [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  466.154877  48.0   \n2     [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  451.912231   0.0   \n3     [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  393.327454  40.0   \n4     [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  180.342300  19.0   \n...                                                 ...         ...   ...   \n9995  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  351.958557  28.0   \n9996  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  323.185333  19.0   \n9997  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  327.438812  49.0   \n9998  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  437.665192  12.0   \n9999  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  355.087891  63.0   \n\n              pt  ieta  \n0     962.311523  16.0  \n1     555.076416  36.0  \n2     434.385803  19.0  \n3     418.650391  21.0  \n4     985.945129  16.0  \n...          ...   ...  \n9995  906.086670  20.0  \n9996  400.424225  15.0  \n9997  704.295532  39.0  \n9998  464.536072  32.0  \n9999  640.220520  17.0  \n\n[10000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_jet</th>\n      <th>m</th>\n      <th>iphi</th>\n      <th>pt</th>\n      <th>ieta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>291.988312</td>\n      <td>33.0</td>\n      <td>962.311523</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>466.154877</td>\n      <td>48.0</td>\n      <td>555.076416</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>451.912231</td>\n      <td>0.0</td>\n      <td>434.385803</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>393.327454</td>\n      <td>40.0</td>\n      <td>418.650391</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>180.342300</td>\n      <td>19.0</td>\n      <td>985.945129</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>351.958557</td>\n      <td>28.0</td>\n      <td>906.086670</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>323.185333</td>\n      <td>19.0</td>\n      <td>400.424225</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>327.438812</td>\n      <td>49.0</td>\n      <td>704.295532</td>\n      <td>39.0</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>437.665192</td>\n      <td>12.0</td>\n      <td>464.536072</td>\n      <td>32.0</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n      <td>355.087891</td>\n      <td>63.0</td>\n      <td>640.220520</td>\n      <td>17.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Formatting and Splitting Data","metadata":{}},{"cell_type":"code","source":"X_train=[]\nfor data in df['X_jet']:\n    val = np.zeros((125,125,3))\n    val[:,:,0]= list(data[0])\n    val[:,:,1]= list(data[3])\n    val[:,:,2]= list(data[1])\n    X_train.append([np.copy(val)])\nX_train = [x[0] for x in X_train]\nX_train = np.array(X_train)\nX_test = X_train[8000:]\nX_train = X_train[:8000]\nY = df['m']\nY_test = Y[8000:]\nY_train = Y[:8000]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:12:39.120205Z","iopub.execute_input":"2024-04-01T09:12:39.120983Z","iopub.status.idle":"2024-04-01T09:12:45.480901Z","shell.execute_reply.started":"2024-04-01T09:12:39.120948Z","shell.execute_reply":"2024-04-01T09:12:45.480039Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of the three channels","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np  # assuming you've imported numpy\n\nfig, ax = plt.subplots(1,3)\nax[0].imshow(X_train[0,:,:,0], cmap='gray')\nax[0].set_title('TrackPt')\nax[1].imshow(X_train[0,:,:,1], cmap='gray')\nax[1].set_title('ECAL')\nax[2].imshow(X_train[0,:,:,2], cmap='gray')\nax[2].set_title('DZ')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:15:28.697080Z","iopub.execute_input":"2024-04-01T09:15:28.697532Z","iopub.status.idle":"2024-04-01T09:15:29.180961Z","shell.execute_reply.started":"2024-04-01T09:15:28.697499Z","shell.execute_reply":"2024-04-01T09:15:29.179951Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 3 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# EfficientNet Implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\n\nclass E2EModel(nn.Module):\n    def __init__(self):\n        super(E2EModel, self).__init__()\n        self.channel1= models.efficientnet_b0(pretrained=True)\n        self.channel1.classifier[1] = nn.Linear(1280,500)\n        self.final = nn.Linear(500,1)\n    def forward(self,x):\n        output1 = self.channel1(x)\n        output = self.final(output1)\n        return output\n# Load pre-trained EfficientNet-B0\nmodel = E2EModel()\n# Print the model\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T08:43:06.089044Z","iopub.execute_input":"2024-04-01T08:43:06.089351Z","iopub.status.idle":"2024-04-01T08:43:06.268398Z","shell.execute_reply.started":"2024-04-01T08:43:06.089326Z","shell.execute_reply":"2024-04-01T08:43:06.267243Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torch import optim\n\ntransform = transforms.Compose([\n    #transforms.RandomCrop(32, padding=4),\n    #transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass MyDataset(Dataset):\n    def __init__(self, X_train,y, transform=None):\n        self.transform = transform\n        self.df = df\n        self.X = X_train\n        self.Y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        image = self.X[idx]\n        \n        label = self.Y[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# Now you can use your custom dataset\ntrainset = MyDataset(X_train,Y_train, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = model.to(device)  # Move model to GPU if available\n\n# Initialize optimizer\noptimizer = optim.Adam(model.parameters())\n\n# Initialize loss function\nloss_fn = nn.MSELoss()\n\n# Number of epochs\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    epoch_loss = 0  # Initialize loss for this epoch\n    num_batches = 0 \n    for i, data in enumerate(trainloader):\n        # Get the inputs; data is a list of [inputs, labels]\n        inputs = data[0].float().to(device)\n        label = data[1].float().to(device)\n         # Zero the parameter gradients\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(inputs)\n        outputs = outputs.squeeze(1)  # This will change the shape from [40, 1] to [40]\n\n        # Compute loss\n        loss = loss_fn(outputs,label)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        num_batches += 1\n        # Print statistics\n    print(f'Loss at the end of epoch {epoch+1}: {epoch_loss/num_batches}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T08:43:08.003973Z","iopub.execute_input":"2024-04-01T08:43:08.004640Z","iopub.status.idle":"2024-04-01T08:53:25.993882Z","shell.execute_reply.started":"2024-04-01T08:43:08.004603Z","shell.execute_reply":"2024-04-01T08:53:25.992951Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Loss at the end of epoch 1: 22022.4250078125\nLoss at the end of epoch 2: 9574.8254453125\nLoss at the end of epoch 3: 8514.6040625\nLoss at the end of epoch 4: 7729.8505546875\nLoss at the end of epoch 5: 7128.15035546875\nLoss at the end of epoch 6: 6346.367015625\nLoss at the end of epoch 7: 5752.9914453125\nLoss at the end of epoch 8: 5111.805760742188\nLoss at the end of epoch 9: 4505.50558984375\nLoss at the end of epoch 10: 3952.580875\nLoss at the end of epoch 11: 3509.00126171875\nLoss at the end of epoch 12: 3147.5509111328124\nLoss at the end of epoch 13: 2879.2652626953127\nLoss at the end of epoch 14: 2723.7088056640623\nLoss at the end of epoch 15: 2567.1792822265625\nLoss at the end of epoch 16: 1945.57934765625\nLoss at the end of epoch 17: 1915.959046875\nLoss at the end of epoch 18: 1839.2395146484375\nLoss at the end of epoch 19: 1734.22871484375\nLoss at the end of epoch 20: 1467.5809809570312\nLoss at the end of epoch 21: 1584.8919399414062\nLoss at the end of epoch 22: 1669.4611948242186\nLoss at the end of epoch 23: 1652.2610649414062\nLoss at the end of epoch 24: 1749.3497348632814\nLoss at the end of epoch 25: 1869.9887397460936\nLoss at the end of epoch 26: 1725.6615903320312\nLoss at the end of epoch 27: 1438.9137377929687\nLoss at the end of epoch 28: 1241.6479775390626\nLoss at the end of epoch 29: 1373.4953129882813\nLoss at the end of epoch 30: 1486.293525390625\nLoss at the end of epoch 31: 1381.9332426757812\nLoss at the end of epoch 32: 1188.9837670898437\nLoss at the end of epoch 33: 1458.365130859375\nLoss at the end of epoch 34: 1332.610604736328\nLoss at the end of epoch 35: 1359.2959296875\nLoss at the end of epoch 36: 1178.9982153320314\nLoss at the end of epoch 37: 1195.4740483398436\nLoss at the end of epoch 38: 1386.7685119628907\nLoss at the end of epoch 39: 1229.1528193359375\nLoss at the end of epoch 40: 1088.5600310058594\nLoss at the end of epoch 41: 1151.5236752929688\nLoss at the end of epoch 42: 872.5763967285156\nLoss at the end of epoch 43: 843.3377866210938\nLoss at the end of epoch 44: 1453.356169189453\nLoss at the end of epoch 45: 1264.2583542480468\nLoss at the end of epoch 46: 1362.1938967285157\nLoss at the end of epoch 47: 1265.2561401367188\nLoss at the end of epoch 48: 982.2872807617188\nLoss at the end of epoch 49: 908.1436330566406\nLoss at the end of epoch 50: 906.5201567382812\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing Model","metadata":{}},{"cell_type":"code","source":"class MyDataset1(Dataset):\n    def __init__(self, X_test,Y_test, transform=None):\n        self.transform = transform\n        self.X = X_test\n        self.Y = Y_test\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        image = self.X[idx]\n        \n        label = self.Y[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# Now you can use your custom dataset\ntestset = MyDataset1(X_test,df1, transform=transform)\ntestloader = DataLoader(testset, batch_size=64, shuffle=True)\n\n# Initialize a list to store the model's predictions\npredictions = []\n\n# Initialize a list to store the true labels\ntrue_labels = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# No need to track gradients in test mode\nwith torch.no_grad():\n    for data in testloader:\n        # Get the inputs; data is a list of [inputs, labels]\n        inputs = data[0].float().to(device)\n        labels = data[1].float().to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n        outputs = outputs.squeeze(1)  # This will change the shape from [40, 1] to [40]\n\n        # Store the model's predictions\n        predictions.extend(outputs.tolist())\n\n        # Store the true labels\n        true_labels.extend(labels.tolist())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T08:59:52.682174Z","iopub.execute_input":"2024-04-01T08:59:52.682546Z","iopub.status.idle":"2024-04-01T08:59:53.820335Z","shell.execute_reply.started":"2024-04-01T08:59:52.682515Z","shell.execute_reply":"2024-04-01T08:59:53.819305Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n\nmae = mean_absolute_error(df1,predictions)\nmse = mean_squared_error(df1,predictions)\nrmse = np.sqrt(mse)\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T08:59:53.821891Z","iopub.execute_input":"2024-04-01T08:59:53.822213Z","iopub.status.idle":"2024-04-01T08:59:53.830495Z","shell.execute_reply.started":"2024-04-01T08:59:53.822187Z","shell.execute_reply":"2024-04-01T08:59:53.829616Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Mean Absolute Error (MAE): 125.69911757278443\nMean Squared Error (MSE): 23534.958244799964\nRoot Mean Squared Error (RMSE): 153.41107601734618\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the entire model\n\n# Save only the model parameters (recommended)\ntorch.save(model.state_dict(), 'regressionparams.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:00:45.236361Z","iopub.execute_input":"2024-04-01T09:00:45.237357Z","iopub.status.idle":"2024-04-01T09:00:45.308383Z","shell.execute_reply.started":"2024-04-01T09:00:45.237318Z","shell.execute_reply":"2024-04-01T09:00:45.307360Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Another MultiLayer Implementation that didnt work","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define the CNN model with three channels\nclass MultiChannelCNN(nn.Module):\n    def __init__(self, input_shape, num_classes):\n        super(MultiChannelCNN, self).__init__()\n        self.conv1_channel1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        self.conv1_channel2 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        self.conv1_channel3 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        \n        self.conv2_channel1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n        self.conv2_channel2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n        self.conv2_channel3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n        \n        self.conv3_channel1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n        self.conv3_channel2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n        self.conv3_channel3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n        \n        self.fc_channel1 = nn.Linear(64 * 27 * 27, 64)\n        self.fc_channel2 = nn.Linear(64 * 27 * 27, 64)\n        self.fc_channel3 = nn.Linear(64 * 27 * 27, 64)\n        \n        self.fc_final1 = nn.Linear(64 * 3, 64)\n        self.fc_final2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        # Forward pass for first channel\n        x_channel1 = x[:,0,:,:]\n        x_channel2 = x[:,1,:,:]\n        x_channel3 = x[:,2,:,:]\n        x_channel1 = x_channel1.unsqueeze(1)\n        x_channel2 = x_channel2.unsqueeze(1)\n        x_channel3 = x_channel3.unsqueeze(1)\n\n        x1 = F.relu(F.max_pool2d(self.conv1_channel1(x_channel1), 2))\n        x1 = F.relu(F.max_pool2d(self.conv2_channel1(x1), 2))\n        x1 = F.relu(self.conv3_channel1(x1))\n        x1 = x1.view(-1, 64 * 27 * 27)  # Flatten\n        \n        # Forward pass for second channel\n        x2 = F.relu(F.max_pool2d(self.conv1_channel2(x_channel2), 2))\n        x2 = F.relu(F.max_pool2d(self.conv2_channel2(x2), 2))\n        x2 = F.relu(self.conv3_channel2(x2))\n        x2 = x2.view(-1, 64 * 27 * 27)  # Flatten\n        \n        # Forward pass for third channel\n        x3 = F.relu(F.max_pool2d(self.conv1_channel3(x_channel3), 2))\n        x3 = F.relu(F.max_pool2d(self.conv2_channel3(x3), 2))\n        x3 = F.relu(self.conv3_channel3(x3))\n        x3 = x3.view(-1, 64 * 27 * 27)  # Flatten\n        \n        # Fully connected layers for each channel\n        x1 = F.relu(self.fc_channel1(x1))\n        x2 = F.relu(self.fc_channel2(x2))\n        x3 = F.relu(self.fc_channel3(x3))\n        \n        # Concatenate the outputs from the three channels\n        x = torch.cat((x1, x2, x3), dim=1)\n        \n        # Final fully connected layers for regression\n        x = F.relu(self.fc_final1(x))\n        x = self.fc_final2(x)\n        return x\n\n# Example input shape for 125x125 images with 3 channels\ninput_shape = (3, 125, 125)\nnum_classes = 1  # Regression task, so output is a single value\n\n# Create the model\nmodel = MultiChannelCNN(input_shape, num_classes)\n\n# Display model architecture\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this approach I tried to apply computations on all three channels separately and then at end tried to concatenate them to give a final output, Howeverit seems that there was a relation between the channels which was ignored by this implemnetation so it didnt work.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}