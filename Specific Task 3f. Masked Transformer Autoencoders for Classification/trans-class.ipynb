{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7946159,"sourceType":"datasetVersion","datasetId":4672376}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries and Formatting Data","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModel\n\ndf = pd.read_csv('/kaggle/input/lollol/HIGGS.csv/HIGGS.csv')\nprint(len(df))\nclass CustomDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n# Split the dataframe into training and testing datasets\ntrain_df = df.iloc[:1100000]\ntest_df = df.iloc[-100000:]  # The last 100k rows\n\ntrain_features = train_df.iloc[:, 1:22].values\ntrain_targets = train_df.iloc[:,0].values\n\ntest_features = test_df.iloc[:, 1:22].values\ntest_targets = test_df.iloc[:,0].values\n\ntrain_dataset = CustomDataset(torch.from_numpy(train_features).float(), torch.from_numpy(train_targets).long())\ntest_dataset = CustomDataset(torch.from_numpy(test_features).float(), torch.from_numpy(test_targets).long())\n\nbatch_size=64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T10:03:30.034911Z","iopub.execute_input":"2024-04-01T10:03:30.035574Z","iopub.status.idle":"2024-04-01T10:06:15.417033Z","shell.execute_reply.started":"2024-04-01T10:03:30.035544Z","shell.execute_reply":"2024-04-01T10:06:15.415856Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"10999999\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Transformer autoencoder for classification","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_auc_score\n\nclass TransformerAutoencoder(nn.Module):\n    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers):\n        super(TransformerAutoencoder, self).__init__()\n        self.input_layer = nn.Linear(input_dim, d_model)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_encoder_layers)\n        \n        # Define the Transformer decoder layer\n        self.decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, batch_first=True)\n        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers)\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(d_model,20),\n            nn.ReLU(),\n            nn.Linear(20,1),\n            nn.Sigmoid()  # Sigmoid activation for pixel values between 0 and 1\n        )\n\n    def forward(self, x):\n        src = self.input_layer(x)\n        memory = self.transformer_encoder(x)\n        decoded = self.transformer_decoder(src,memory)\n        return self.decoder(decoded)\n\n\n# Hyperparameters\ninput_dim = 21\nd_model = 21\n0\nnhead =7\nnum_encoder_layers = 1\nnum_decoder_layers = 1\nnum_epochs = 50\nlearning_rate = 0.001\nbatch_size = 64\n\n# Initialize the model and move it to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = TransformerAutoencoder(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers).to(device)\nmodel.use_nested_tensor = True\n\n# Loss and optimizer\ncriterion =nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\n# Training loop\nfor epoch in range(num_epochs):\n    epoch_loss=0.0\n    for i, (data, labels) in enumerate(train_loader):\n        data = data.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(data)\n\n        loss = criterion(outputs[:,0], labels.float())\n        epoch_loss += loss.item()\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n\n\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    y_true = []\n    y_pred = []\n    for data, labels in test_loader:\n        data = data.to(device)\n        labels = labels.to(device)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\nprint('ROC-AUC score: ', roc_auc_score(y_true, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T10:38:31.098878Z","iopub.execute_input":"2024-04-01T10:38:31.099227Z","iopub.status.idle":"2024-04-01T12:21:12.522228Z","shell.execute_reply.started":"2024-04-01T10:38:31.099204Z","shell.execute_reply":"2024-04-01T12:21:12.521275Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch [1/50], Step [17188/17188], Loss: 0.62662672996521\nEpoch [2/50], Step [17188/17188], Loss: 0.6776449680328369\nEpoch [3/50], Step [17188/17188], Loss: 0.6665347814559937\nEpoch [4/50], Step [17188/17188], Loss: 0.6695046424865723\nEpoch [5/50], Step [17188/17188], Loss: 0.629170298576355\nEpoch [6/50], Step [17188/17188], Loss: 0.6439266800880432\nEpoch [7/50], Step [17188/17188], Loss: 0.618904173374176\nEpoch [8/50], Step [17188/17188], Loss: 0.7013390064239502\nEpoch [9/50], Step [17188/17188], Loss: 0.6393381357192993\nEpoch [10/50], Step [17188/17188], Loss: 0.5353981852531433\nEpoch [11/50], Step [17188/17188], Loss: 0.5754103660583496\nEpoch [12/50], Step [17188/17188], Loss: 0.6596124172210693\nEpoch [13/50], Step [17188/17188], Loss: 0.6680445671081543\nEpoch [14/50], Step [17188/17188], Loss: 0.6128355860710144\nEpoch [15/50], Step [17188/17188], Loss: 0.6691184043884277\nEpoch [16/50], Step [17188/17188], Loss: 0.6345705986022949\nEpoch [17/50], Step [17188/17188], Loss: 0.6168990731239319\nEpoch [18/50], Step [17188/17188], Loss: 0.6394538879394531\nEpoch [19/50], Step [17188/17188], Loss: 0.6331325769424438\nEpoch [20/50], Step [17188/17188], Loss: 0.6486350893974304\nEpoch [21/50], Step [17188/17188], Loss: 0.7103835344314575\nEpoch [22/50], Step [17188/17188], Loss: 0.6138168573379517\nEpoch [23/50], Step [17188/17188], Loss: 0.6557946801185608\nEpoch [24/50], Step [17188/17188], Loss: 0.5947120785713196\nEpoch [25/50], Step [17188/17188], Loss: 0.6666638851165771\nEpoch [26/50], Step [17188/17188], Loss: 0.5734158754348755\nEpoch [27/50], Step [17188/17188], Loss: 0.6217679977416992\nEpoch [28/50], Step [17188/17188], Loss: 0.6397879719734192\nEpoch [29/50], Step [17188/17188], Loss: 0.722139835357666\nEpoch [30/50], Step [17188/17188], Loss: 0.6923314332962036\nEpoch [31/50], Step [17188/17188], Loss: 0.6996207237243652\nEpoch [32/50], Step [17188/17188], Loss: 0.6331284642219543\nEpoch [33/50], Step [17188/17188], Loss: 0.6588315963745117\nEpoch [34/50], Step [17188/17188], Loss: 0.5538635849952698\nEpoch [35/50], Step [17188/17188], Loss: 0.6566060781478882\nEpoch [36/50], Step [17188/17188], Loss: 0.6482428312301636\nEpoch [37/50], Step [17188/17188], Loss: 0.6025382280349731\nEpoch [38/50], Step [17188/17188], Loss: 0.615776538848877\nEpoch [39/50], Step [17188/17188], Loss: 0.6677871942520142\nEpoch [40/50], Step [17188/17188], Loss: 0.6615760326385498\nEpoch [41/50], Step [17188/17188], Loss: 0.6421797275543213\nEpoch [42/50], Step [17188/17188], Loss: 0.6272087097167969\nEpoch [43/50], Step [17188/17188], Loss: 0.6237223744392395\nEpoch [44/50], Step [17188/17188], Loss: 0.6800110340118408\nEpoch [45/50], Step [17188/17188], Loss: 0.5522933006286621\nEpoch [46/50], Step [17188/17188], Loss: 0.5547680854797363\nEpoch [47/50], Step [17188/17188], Loss: 0.5643259286880493\nEpoch [48/50], Step [17188/17188], Loss: 0.7212238311767578\nEpoch [49/50], Step [17188/17188], Loss: 0.6801211833953857\nEpoch [50/50], Step [17188/17188], Loss: 0.6714307069778442\nROC-AUC score:  0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T08:51:16.118099Z","iopub.execute_input":"2024-04-01T08:51:16.118500Z","iopub.status.idle":"2024-04-01T08:51:16.157060Z","shell.execute_reply.started":"2024-04-01T08:51:16.118470Z","shell.execute_reply":"2024-04-01T08:51:16.156037Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"I tried several auto encoder architectures including Transformer decoder, simple linear hidden unit decoders and single hidden layer decoders. The transformer decoder proved to be the best.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}